diff --git a/configs/nice/nice_ssm_vr.yml b/configs/nice/nice_ssm_vr.yml
index 4b9af35..e6f7bb6 100644
--- a/configs/nice/nice_ssm_vr.yml
+++ b/configs/nice/nice_ssm_vr.yml
@@ -7,7 +7,7 @@ data:
   dataset: "MNIST"
   image_size: 28
   channels: 1
-  noise_sigma: 0.0
+  noise_sigma: 0.1
 model:
   hidden_size: 1000
   num_layers: 5 # only 2 or 5
diff --git a/evaluations/fid.py b/evaluations/fid.py
index 8e1a5a6..eed54d5 100644
--- a/evaluations/fid.py
+++ b/evaluations/fid.py
@@ -39,7 +39,7 @@ from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
 import numpy as np
 import torch
 from scipy import linalg
-from scipy.misc import imread
+from imageio import imread
 from torch.nn.functional import adaptive_avg_pool2d
 
 try:
diff --git a/main.py b/main.py
index a840018..6e14e4c 100644
--- a/main.py
+++ b/main.py
@@ -35,7 +35,7 @@ def parse_args_and_config():
 
     # parse config file
     with open(os.path.join('configs', args.config), 'r') as f:
-        config = yaml.load(f)
+        config = yaml.safe_load(f)
     new_config = dict2namespace(config)
 
     if not args.test:
diff --git a/runners/nice_runner.py b/runners/nice_runner.py
index b63f055..22cfd5d 100644
--- a/runners/nice_runner.py
+++ b/runners/nice_runner.py
@@ -50,10 +50,7 @@ class NICERunner():
         self.results[model_type] = {}
 
         for i, (X, y) in enumerate(val_loader):
-            X = X + (torch.rand_like(X) - 0.5) / 256.
             flattened_X = X.type(torch.float32).to(self.config.device).view(X.shape[0], -1)
-            flattened_X.clamp_(1e-3, 1-1e-3)
-            flattened_X, _ = Logit()(flattened_X, mode='direct')
             if noise_sigma is not None:
                 flattened_X += torch.randn_like(flattened_X) * noise_sigma
 
@@ -74,10 +71,7 @@ class NICERunner():
         test_sm_losses = []
 
         for i, (X, y) in enumerate(test_loader):
-            X = X + (torch.rand_like(X) - 0.5) / 256.
             flattened_X = X.type(torch.float32).to(self.config.device).view(X.shape[0], -1)
-            flattened_X.clamp_(1e-3, 1-1e-3)
-            flattened_X, _ = Logit()(flattened_X, mode='direct')
             if noise_sigma is not None:
                 flattened_X += torch.randn_like(flattened_X) * noise_sigma
 
@@ -106,17 +100,17 @@ class NICERunner():
         elif self.config.data.dataset == 'MNIST':
             dataset = MNIST(os.path.join(self.args.run, 'datasets', 'mnist'), train=True, download=True,
                             transform=transform)
-            num_items = len(dataset)
-            indices = list(range(num_items))
-            random_state = np.random.get_state()
-            np.random.seed(2019)
-            np.random.shuffle(indices)
-            np.random.set_state(random_state)
-            train_indices, val_indices = indices[:int(num_items * 0.9)], indices[int(num_items * 0.9):]
-            val_dataset = Subset(dataset, val_indices)
-            dataset = Subset(dataset, train_indices)
             test_dataset = MNIST(os.path.join(self.args.run, 'datasets', 'mnist'), train=False, download=True,
                                  transform=transform)
+        num_items = len(dataset)
+        indices = list(range(num_items))
+        random_state = np.random.get_state()
+        np.random.seed(2019)
+        np.random.shuffle(indices)
+        np.random.set_state(random_state)
+        train_indices, val_indices = indices[:int(num_items * 0.9)], indices[int(num_items * 0.9):]
+        val_dataset = Subset(dataset, val_indices)
+        dataset = Subset(dataset, train_indices)
 
         dataloader = DataLoader(dataset, batch_size=self.config.training.batch_size, shuffle=True, num_workers=2)
         val_loader = DataLoader(val_dataset, batch_size=self.config.training.batch_size, shuffle=True,
@@ -169,7 +163,7 @@ class NICERunner():
 
         def sample_net(z):
             samples, _ = flow(z, inv=True)
-            samples, _ = Logit()(samples, mode='inverse')
+            #samples, _ = Logit()(samples, mode='inverse')
             return samples
 
         # Use this to select the sigma for DSM losses
@@ -189,10 +183,7 @@ class NICERunner():
 
         for _ in range(self.config.training.n_epochs):
             for _, (X, y) in enumerate(dataloader):
-                X = X + (torch.rand_like(X) - 0.5) / 256.
                 flattened_X = X.type(torch.float32).to(self.config.device).view(X.shape[0], -1)
-                flattened_X.clamp_(1e-3, 1-1e-3)
-                flattened_X, _ = Logit()(flattened_X, mode='direct')
 
                 if noise_sigma is not None:
                     flattened_X += torch.randn_like(flattened_X) * noise_sigma
@@ -232,10 +223,7 @@ class NICERunner():
                         val_iter = iter(val_loader)
                         val_X, _ = next(val_iter)
 
-                    val_X = val_X + (torch.rand_like(val_X) - 0.5) / 256.
                     val_X = val_X.type(torch.float32).to(self.config.device)
-                    val_X.clamp_(1e-3, 1-1e-3)
-                    val_X, _ = Logit()(val_X, mode='direct')
                     val_X = val_X.view(val_X.shape[0], -1)
                     if noise_sigma is not None:
                         val_X += torch.randn_like(val_X) * noise_sigma
@@ -296,10 +284,7 @@ class NICERunner():
                         val_iter = iter(val_loader)
                         val_X, _ = next(val_iter)
 
-                    val_X = val_X + (torch.rand_like(val_X) - 0.5) / 256.
                     val_X = val_X.type(torch.float32).to(self.config.device)
-                    val_X.clamp_(1e-3, 1-1e-3)
-                    val_X, _ = Logit()(val_X, mode='direct')
                     val_X = val_X.view(val_X.shape[0], -1)
                     if noise_sigma is not None:
                         val_X += torch.randn_like(val_X) * noise_sigma
